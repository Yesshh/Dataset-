{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brain Tumor MP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8QRk1LitcsyhSJG2NRL1f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yesshh/Dataset-/blob/main/Brain_Tumor_MP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tjKoVoRW8fvX"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_exPMl8C8zIR",
        "outputId": "0a7cd5ce-4474-4812-c483-43d618d29feb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.64.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (6.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.10.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od"
      ],
      "metadata": {
        "id": "DU0Y9T_y8y2P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/ahmedhamada0/brain-tumor-detection\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPfz7GZMAPD7",
        "outputId": "cd981d45-1a8a-48f1-92c3-6d9061afadad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: yesh25\n",
            "Your Kaggle Key: ··········\n",
            "Downloading brain-tumor-detection.zip to ./brain-tumor-detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.0M/84.0M [00:00<00:00, 157MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "import shutil\n",
        "import glob\n",
        " "
      ],
      "metadata": {
        "id": "AjgK83PlAXGg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6bdklnY4BQFN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count number of images\n",
        "ROOT_DIR = \"/content/brain-tumor-detection\"\n",
        "number_of_images = {}\n",
        "\n",
        "for dir in os.listdir(ROOT_DIR):\n",
        "  number_of_images[dir] = len(os.listdir(os.path.join(ROOT_DIR, dir)))\n",
        "\n",
        "number_of_images.items() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ0P_W-523aw",
        "outputId": "9ea38a0e-3191-41c7-9299-c5419595d4d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('yes', 1500), ('pred', 60), ('no', 1500), ('Br35H-Mask-RCNN', 4)])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MX3ysXmUBaSQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataFolder(p, split):\n",
        "  if not os.path.exists(\"./\"+p):\n",
        "    os.mkdir(\"./\"+p)\n",
        "\n",
        "\n",
        "    for dir in os.listdir(ROOT_DIR):\n",
        "      os.makedirs(\"./\"+p+\"/\"+dir)\n",
        "    \n",
        "    \n",
        "      for img in np.random.choice(a = os.listdir(os.path.join(ROOT_DIR, dir)) ,\n",
        "                                size = (math.floor(split*number_of_images[dir])-2), \n",
        "                                replace=False ):\n",
        "          O = os.path.join(ROOT_DIR,dir,img) #path\n",
        "          D = os.path.join(\"./\"+p,dir) \n",
        "          shutil.copy(O,D)\n",
        "          os.remove(O)\n",
        "\n",
        "  else:\n",
        "    print(f\"{p} folder exist\")\n"
      ],
      "metadata": {
        "id": "OUMFTKA_6C08"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFolder(\"train\",0.7)"
      ],
      "metadata": {
        "id": "oE7XgPitCgEp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFolder(\"val\",0.15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks7L_IXXCmSb",
        "outputId": "7fdd5230-11f8-4443-fd24-fac0d3e800f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val folder exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataFolder(\"test\",0.15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmdcEWCEC-Nx",
        "outputId": "c6be86a8-443b-42d4-d8e9-5b511659f641"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test folder exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_images = {}\n",
        "\n",
        "for dir in os.listdir(ROOT_DIR):\n",
        "  number_of_images[dir] = len(os.listdir(os.path.join(ROOT_DIR, dir)))\n",
        "\n",
        "number_of_images.items() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_71lqODDE1D",
        "outputId": "ef7d100a-21eb-4555-f69b-c994f187b1bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('yes', 229), ('pred', 13), ('no', 229), ('Br35H-Mask-RCNN', 4)])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL BUILD\n"
      ],
      "metadata": {
        "id": "6vIpCIzfDbpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, BatchNormalization, GlobalAvgPool2D\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "import keras"
      ],
      "metadata": {
        "id": "C82c40-ZDT9z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN Model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=16, kernel_size=(3,3),activation= 'relu', input_shape = (224,224,3)))\n",
        "\n",
        "model.add(Conv2D(filters=36, kernel_size=(3,3),activation= 'relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3),activation= 'relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3),activation= 'relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout(rate= 0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dropout(rate= 0.25))\n",
        "model.add(Dense(units= 1, activation= 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tugJbgeADfAy",
        "outputId": "d218c5c0-7c30-4d0f-ca4d-592048af5f37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 222, 222, 16)      448       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 220, 220, 36)      5220      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 110, 110, 36)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 108, 108, 64)      20800     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 52, 52, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 26, 26, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 86528)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                5537856   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,638,245\n",
            "Trainable params: 5,638,245\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss= keras.losses.binary_crossentropy, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "MbrQcK7gDiqw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing our data using data generator"
      ],
      "metadata": {
        "id": "56UX9ARgDn5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessingImages1(path):\n",
        "  \"\"\"\n",
        "  imput : path\n",
        "  output : Pre processed images\n",
        "  \"\"\"\n",
        "\n",
        "  image_data = ImageDataGenerator(zoom_range=0.2, shear_range= 0.2, rescale=1/255, horizontal_flip= True)\n",
        "  image = image_data.flow_from_directory(directory = path, target_size=(224,224), batch_size=32, class_mode='binary')\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "hI0tXj7uDmUN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path  = \"/content/train\"\n",
        "train_data = preprocessingImages1(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4pzyDnWDt2Q",
        "outputId": "1d263434-0382-4b0b-9502-b3fae37ed300"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2136 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessingImages2(path):\n",
        "  \"\"\"\n",
        "  imput : path\n",
        "  output : Pre processed images\n",
        "  \"\"\"\n",
        "\n",
        "  image_data = ImageDataGenerator(rescale=1/255, )\n",
        "  image = image_data.flow_from_directory(directory = path, target_size=(224,224), batch_size=32, class_mode='binary')\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "WxlN5pf6Dxgs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/test\"\n",
        "test_data = preprocessingImages2(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdnOhLXnDz_3",
        "outputId": "1353e889-e26e-42ac-c6e1-ace0ff333c93"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 453 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/val\"\n",
        "val_data = preprocessingImages2(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRO16jVpD2gE",
        "outputId": "f67ab004-eef7-4f6e-d1f2-a26edb6f2425"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping and model check point\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# early stopping\n",
        "\n",
        "es = EarlyStopping(monitor=\"val_accuracy\", min_delta=0.01, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "# model check point\n",
        "\n",
        "mc = ModelCheckpoint(monitor=\"val_accuracy\", filepath=\"./bestmodel.h5\", verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "cd = [es,mc]\n"
      ],
      "metadata": {
        "id": "AaBIRvacEDgR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hs = model.fit_generator(generator= train_data, \n",
        "                         steps_per_epoch=8, \n",
        "                         epochs= 30, \n",
        "                         verbose= 1, \n",
        "                         validation_data=val_data, \n",
        "                         validation_steps= 16, \n",
        "                         callbacks= cd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lluO1m61EGPw",
        "outputId": "bada50c8-da29-40c5-b271-d01a0b38d0be"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -365.0779 - accuracy: 0.4531WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 16s 463ms/step - loss: -365.0779 - accuracy: 0.4531\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -16387.8008 - accuracy: 0.4336WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 500ms/step - loss: -16387.8008 - accuracy: 0.4336\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -204433.9844 - accuracy: 0.4492WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 493ms/step - loss: -204433.9844 - accuracy: 0.4492\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -1324171.3750 - accuracy: 0.4922WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 501ms/step - loss: -1324171.3750 - accuracy: 0.4922\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -7485507.0000 - accuracy: 0.4844WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 508ms/step - loss: -7485507.0000 - accuracy: 0.4844\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -31874152.0000 - accuracy: 0.4492WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 508ms/step - loss: -31874152.0000 - accuracy: 0.4492\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -106135488.0000 - accuracy: 0.4688WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 500ms/step - loss: -106135488.0000 - accuracy: 0.4688\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -341099456.0000 - accuracy: 0.4648WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 503ms/step - loss: -341099456.0000 - accuracy: 0.4648\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -813153088.0000 - accuracy: 0.5195WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 491ms/step - loss: -813153088.0000 - accuracy: 0.5195\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -2011440896.0000 - accuracy: 0.4766WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 508ms/step - loss: -2011440896.0000 - accuracy: 0.4766\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -4416733184.0000 - accuracy: 0.5039WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 507ms/step - loss: -4416733184.0000 - accuracy: 0.5039\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -8851230720.0000 - accuracy: 0.5121WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 5s 577ms/step - loss: -8851230720.0000 - accuracy: 0.5121\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -19193286656.0000 - accuracy: 0.4453WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 508ms/step - loss: -19193286656.0000 - accuracy: 0.4453\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -33997758464.0000 - accuracy: 0.4922WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 501ms/step - loss: -33997758464.0000 - accuracy: 0.4922\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -56078307328.0000 - accuracy: 0.4883WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 498ms/step - loss: -56078307328.0000 - accuracy: 0.4883\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -89230753792.0000 - accuracy: 0.5121WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 483ms/step - loss: -89230753792.0000 - accuracy: 0.5121\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -155561754624.0000 - accuracy: 0.4961WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 503ms/step - loss: -155561754624.0000 - accuracy: 0.4961\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -284524871680.0000 - accuracy: 0.4531WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 500ms/step - loss: -284524871680.0000 - accuracy: 0.4531\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -366791393280.0000 - accuracy: 0.5312WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 502ms/step - loss: -366791393280.0000 - accuracy: 0.5312\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -652124618752.0000 - accuracy: 0.4766WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 508ms/step - loss: -652124618752.0000 - accuracy: 0.4766\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -981174845440.0000 - accuracy: 0.4637WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 501ms/step - loss: -981174845440.0000 - accuracy: 0.4637\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -1580068896768.0000 - accuracy: 0.4113WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 497ms/step - loss: -1580068896768.0000 - accuracy: 0.4113\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -1874741821440.0000 - accuracy: 0.4844WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 500ms/step - loss: -1874741821440.0000 - accuracy: 0.4844\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -3250678398976.0000 - accuracy: 0.3984WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 515ms/step - loss: -3250678398976.0000 - accuracy: 0.3984\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -3399202897920.0000 - accuracy: 0.5156WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 505ms/step - loss: -3399202897920.0000 - accuracy: 0.5156\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -5216602685440.0000 - accuracy: 0.4637WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 492ms/step - loss: -5216602685440.0000 - accuracy: 0.4637\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -6057034776576.0000 - accuracy: 0.5234WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 503ms/step - loss: -6057034776576.0000 - accuracy: 0.5234\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -9469576609792.0000 - accuracy: 0.4570WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 510ms/step - loss: -9469576609792.0000 - accuracy: 0.4570\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -12556966035456.0000 - accuracy: 0.4395WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 511ms/step - loss: -12556966035456.0000 - accuracy: 0.4395\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - ETA: 0s - loss: -15402818076672.0000 - accuracy: 0.4805WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "8/8 [==============================] - 4s 509ms/step - loss: -15402818076672.0000 - accuracy: 0.4805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model graphical interpretation\n",
        "\n",
        "h = hs.history\n",
        "h.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4DveAaREIZC",
        "outputId": "96688dc4-a7a4-4833-ab03-115e850e26e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(h['accuracy'])\n",
        "plt.plot(h['loss'], c=\"red\")\n",
        "\n",
        "plt.title(\"acc vs val-acc\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "padHozbgFzd6",
        "outputId": "d027ba26-fd01-402a-c8ff-5bf5b1d239ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yvNiNbPMF2L8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}